{"componentChunkName":"component---src-pages-troubleshooting-operator-upgrades-mdx","path":"/troubleshooting/operator-upgrades/","result":{"pageContext":{"frontmatter":{"title":"DataPower Operator Upgrades","description":"Troubleshooting guide related to upgrading the DataPower Operator"},"relativePagePath":"/troubleshooting/operator-upgrades.mdx","titleType":"page","MdxNode":{"id":"3480e2b3-517a-5fd2-a350-6460afdb2636","children":[],"parent":"a3ebfe7a-9e4b-59eb-932a-1dc3e3128535","internal":{"content":"---\ntitle: DataPower Operator Upgrades\ndescription: Troubleshooting guide related to upgrading the DataPower Operator\n---\n\n<PageDescription>\n\nIf any known issues exist or migration paths are needed to upgrade from one version of the DataPower Operator to another, they will be documented here.\n\n</PageDescription>\n\n# 1.2.0\n\n## Pod Topology Spread Constraints\n\n### Symptoms\n\nDataPower Operator pods fail to schedule, stating that no nodes match pod topology spread constraints (missing required label).\n\n```\n0/15 nodes are available: 12 node(s) didn't match pod topology spread constraints (missing required label), 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.\n```\n\n### Description\n\nThere was a [Kubernetes bug](https://github.com/kubernetes/kubernetes/issues/95808) that allowed for pods to schedule when `topologyKey` was not matched. Kubernetes recently fixed this bug in 1.20, and back-ported the fix to 1.18 and 1.19.\n\nOnce this fix is installed to a Kubernetes cluster, the scheduler would no longer schedule DataPower Operator pods, due to the `topologyKey` in our pod spec not using a well-known \"zone\" label.\n\n### Solution\n\n<InlineNotification>\n\nWe have fixed this issue in DataPower Operator version `1.2.1`, see [Release notes](/release-notes#1.2.1).\n\n</InlineNotification>\n\nPerforming a clean installation of DataPower Operator version `1.2.1` or higher should succeed without the below workaround needed. However, if you previously tried to upgrade to `1.2.0` and the upgrade is stuck in a pending state, the following workaround can be used to allow the `1.2.0` install to complete.\n\nManually add a `zone` label (with any value) to one of the worker nodes in the Kubernetes cluster. The DataPower Operator pods will then be scheduled to that worker node.\n\n```\nkubectl label nodes <node-name> zone=<label-value>\n```\n\nReference: [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#step-one-attach-label-to-the-node)\n\nOptionally, once a successful upgrade to `1.2.1` has been completed, the `zone` label can be removed from the node(s) in the cluster. This `zone` label is no longer used in `1.2.1` or higher.\n\n## Slow Kubernetes garbage collection can cause webhook service creation to fail\n\n### Symptoms\n\nAn error may be seen that indicates a webhook `Service` is not found:\n\n```\nfailed calling webhook \"datapowerservices.defaulter.datapower.ibm.com\": Post https://datapower-operator.default.svc:443/default-datapower-ibm-com-v1beta2-datapowerservice?timeout=2s: service \"datapower-operator\" not found\n```\n\nIn the `datapower-operator` pod logs, the following log message is seen indicating the `Service` exists already:\n\n```\n{\"level\":\"info\",\"ts\":\"2020-10-06T20:32:45.818Z\",\"logger\":\"setup-webhook\",\"msg\":\"webhook service found. skip create\"}\n```\n\nHowever, checking for the `Service` object (name `datapower-operator`) in the namespace would show it does not exist:\n\n```\n$ kubectl get svc datapower-operator\n```\n\n### Description\n\nDuring the DataPower Operator boot the defaulting and validating webhooks are initialized. As part of this process, a `Service` object is created if not found in the cluster. If the Kubernetes garbage collection is slow or delayed it is possible for the `Service` object to have been marked for deletion, but still exist in the namespace for a span of time. If the new DataPower Operator pod attempts to initialize the new `Service` resource during this time window, the operator will continue through the boot sequence without creating a new `Service` instance.\n\nOnce the Kubernetes garbage collection catches up, and the `Service` is deleted, no `Service` will remain to expose the webhooks and thus API errors will be seen when invoking the webhooks.\n\n### Solution\n\nTo resolve this issue, the DataPower Operator pod can be deleted manually. Once this is done, the Deployment's ReplicaSet controller will spin up a new DataPower Operator pod, which will in turn create the webhook `Service`.\n\n1. Fetch the DataPower Operator pod, taking note of the name (will be the first column in the output).\n\n    ```\n    kubectl [-n namespace] get pod | grep datapower-operator\n    ```\n\n2. Delete the DataPower Operator pod, where `name` is the name of the pod found in Step 5.\n\n    ```\n    kubectl [-n namespace] delete pod/name\n    ```\n\n3. Validate a new pod is created in its place.\n\n    ```\n    kubectl [-n namespace] get pod | grep datapower-operator\n    ```\n\n# 1.1.0\n\n## Operator lock not released after Leader Pod removed\n\n### Symptoms\n\n- Immediately following an upgrade from 1.0.X to 1.1.X, you may see the following error when attempting to interact with a DataPowerService CR:\n  ```\n  Error from server: conversion webhook for datapower.ibm.com/v1beta1, Kind=DataPowerService failed: Post https://changeme.default.svc:443/?timeout=30s: service \"changeme\" not found\n  ```\n- Failure to reconcile changes made to a DataPowerService CR after a failover event caused the previous DataPower Operator Leader Pod to be rescheduled.\n\n### Description\n\nOccasionally, when the DataPower Operator Leader Pod is removed, the new DataPower Operator Pod will show as being Ready but isn't doing anything. In the logs you might see\n```\n{\"level\":\"info\",\"ts\":\"2020-09-08T19:29:53.432Z\",\"logger\":\"leader\",\"msg\":\"Not the leader. Waiting.\"}\n{\"level\":\"info\",\"ts\":\"2020-09-08T19:29:57.971Z\",\"logger\":\"leader\",\"msg\":\"Leader pod has been deleted, waiting for garbage collection do remove the lock.\"}\n```\nfollowed by additional waiting. In this case, Kubernetes' garbage collection failed to clean up the `datapower-operator-lock` after the old Leader Pod was removed. This stops the new DataPower Operator Pod from completing initialization tasks such as creating the conversion webhook.\n\n### Solution\n\nTo resolve this issue, you must manually remove the lock resource. This can be done with\n```\nkubectl delete cm datapower-operator-lock\n```\nOnce the lock is removed, the new DataPower Operator pod will begin its initialization.\n\n# 1.0.1\n\n## Invalid value for `spec.selector`\n\nWhen attempting to ugprade from `1.0.0` to `1.0.1` through the Operator Lifecycle Manager, an error will likely be seen that the `installPlan` failed.\n\n```\ninstall strategy failed: Deployment.apps \"datapower-operator\" is invalid: spec.selector: Invalid value: v1.LabelSelector{...}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable\n```\n\nThis error occurs because between version `1.0.0` and `1.0.1` a new `label` was added to the DataPower Operator Deployment resource to fix an issue related to operator-metrics. However, the install plan is not able to resolve this update because the LabelSelector is an immutable field.\n\n### Resolution\n\nTo workaround this issue, you can manually delete the existing `datapower-operator` Deployment resource. The Operator Lifecycle Manager should then recreate the `datapower-operator` Deployment resource with the `1.0.1` spec, and the install plan should succeed.\n\n```\noc delete deployment datapower-operator\n```\n\nOnce done, validate a new `datapower-operator` Deployment is created:\n\n```\noc get deployment\n```\n\nThen validate the `1.0.1` install plan succeeds by checking the `ClusterServiceVersion` resource:\n\n```\n$ oc get csv\nNAME                        DISPLAY                 VERSION   REPLACES                    PHASE\ndatapower-operator.v1.0.1   IBM DataPower Gateway   1.0.1     datapower-operator.v1.0.0   Succeeded\n```\n","type":"Mdx","contentDigest":"c9c3caf03264b40f065d82cf833be4e0","counter":130,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"DataPower Operator Upgrades","description":"Troubleshooting guide related to upgrading the DataPower Operator"},"exports":{},"rawBody":"---\ntitle: DataPower Operator Upgrades\ndescription: Troubleshooting guide related to upgrading the DataPower Operator\n---\n\n<PageDescription>\n\nIf any known issues exist or migration paths are needed to upgrade from one version of the DataPower Operator to another, they will be documented here.\n\n</PageDescription>\n\n# 1.2.0\n\n## Pod Topology Spread Constraints\n\n### Symptoms\n\nDataPower Operator pods fail to schedule, stating that no nodes match pod topology spread constraints (missing required label).\n\n```\n0/15 nodes are available: 12 node(s) didn't match pod topology spread constraints (missing required label), 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.\n```\n\n### Description\n\nThere was a [Kubernetes bug](https://github.com/kubernetes/kubernetes/issues/95808) that allowed for pods to schedule when `topologyKey` was not matched. Kubernetes recently fixed this bug in 1.20, and back-ported the fix to 1.18 and 1.19.\n\nOnce this fix is installed to a Kubernetes cluster, the scheduler would no longer schedule DataPower Operator pods, due to the `topologyKey` in our pod spec not using a well-known \"zone\" label.\n\n### Solution\n\n<InlineNotification>\n\nWe have fixed this issue in DataPower Operator version `1.2.1`, see [Release notes](/release-notes#1.2.1).\n\n</InlineNotification>\n\nPerforming a clean installation of DataPower Operator version `1.2.1` or higher should succeed without the below workaround needed. However, if you previously tried to upgrade to `1.2.0` and the upgrade is stuck in a pending state, the following workaround can be used to allow the `1.2.0` install to complete.\n\nManually add a `zone` label (with any value) to one of the worker nodes in the Kubernetes cluster. The DataPower Operator pods will then be scheduled to that worker node.\n\n```\nkubectl label nodes <node-name> zone=<label-value>\n```\n\nReference: [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#step-one-attach-label-to-the-node)\n\nOptionally, once a successful upgrade to `1.2.1` has been completed, the `zone` label can be removed from the node(s) in the cluster. This `zone` label is no longer used in `1.2.1` or higher.\n\n## Slow Kubernetes garbage collection can cause webhook service creation to fail\n\n### Symptoms\n\nAn error may be seen that indicates a webhook `Service` is not found:\n\n```\nfailed calling webhook \"datapowerservices.defaulter.datapower.ibm.com\": Post https://datapower-operator.default.svc:443/default-datapower-ibm-com-v1beta2-datapowerservice?timeout=2s: service \"datapower-operator\" not found\n```\n\nIn the `datapower-operator` pod logs, the following log message is seen indicating the `Service` exists already:\n\n```\n{\"level\":\"info\",\"ts\":\"2020-10-06T20:32:45.818Z\",\"logger\":\"setup-webhook\",\"msg\":\"webhook service found. skip create\"}\n```\n\nHowever, checking for the `Service` object (name `datapower-operator`) in the namespace would show it does not exist:\n\n```\n$ kubectl get svc datapower-operator\n```\n\n### Description\n\nDuring the DataPower Operator boot the defaulting and validating webhooks are initialized. As part of this process, a `Service` object is created if not found in the cluster. If the Kubernetes garbage collection is slow or delayed it is possible for the `Service` object to have been marked for deletion, but still exist in the namespace for a span of time. If the new DataPower Operator pod attempts to initialize the new `Service` resource during this time window, the operator will continue through the boot sequence without creating a new `Service` instance.\n\nOnce the Kubernetes garbage collection catches up, and the `Service` is deleted, no `Service` will remain to expose the webhooks and thus API errors will be seen when invoking the webhooks.\n\n### Solution\n\nTo resolve this issue, the DataPower Operator pod can be deleted manually. Once this is done, the Deployment's ReplicaSet controller will spin up a new DataPower Operator pod, which will in turn create the webhook `Service`.\n\n1. Fetch the DataPower Operator pod, taking note of the name (will be the first column in the output).\n\n    ```\n    kubectl [-n namespace] get pod | grep datapower-operator\n    ```\n\n2. Delete the DataPower Operator pod, where `name` is the name of the pod found in Step 5.\n\n    ```\n    kubectl [-n namespace] delete pod/name\n    ```\n\n3. Validate a new pod is created in its place.\n\n    ```\n    kubectl [-n namespace] get pod | grep datapower-operator\n    ```\n\n# 1.1.0\n\n## Operator lock not released after Leader Pod removed\n\n### Symptoms\n\n- Immediately following an upgrade from 1.0.X to 1.1.X, you may see the following error when attempting to interact with a DataPowerService CR:\n  ```\n  Error from server: conversion webhook for datapower.ibm.com/v1beta1, Kind=DataPowerService failed: Post https://changeme.default.svc:443/?timeout=30s: service \"changeme\" not found\n  ```\n- Failure to reconcile changes made to a DataPowerService CR after a failover event caused the previous DataPower Operator Leader Pod to be rescheduled.\n\n### Description\n\nOccasionally, when the DataPower Operator Leader Pod is removed, the new DataPower Operator Pod will show as being Ready but isn't doing anything. In the logs you might see\n```\n{\"level\":\"info\",\"ts\":\"2020-09-08T19:29:53.432Z\",\"logger\":\"leader\",\"msg\":\"Not the leader. Waiting.\"}\n{\"level\":\"info\",\"ts\":\"2020-09-08T19:29:57.971Z\",\"logger\":\"leader\",\"msg\":\"Leader pod has been deleted, waiting for garbage collection do remove the lock.\"}\n```\nfollowed by additional waiting. In this case, Kubernetes' garbage collection failed to clean up the `datapower-operator-lock` after the old Leader Pod was removed. This stops the new DataPower Operator Pod from completing initialization tasks such as creating the conversion webhook.\n\n### Solution\n\nTo resolve this issue, you must manually remove the lock resource. This can be done with\n```\nkubectl delete cm datapower-operator-lock\n```\nOnce the lock is removed, the new DataPower Operator pod will begin its initialization.\n\n# 1.0.1\n\n## Invalid value for `spec.selector`\n\nWhen attempting to ugprade from `1.0.0` to `1.0.1` through the Operator Lifecycle Manager, an error will likely be seen that the `installPlan` failed.\n\n```\ninstall strategy failed: Deployment.apps \"datapower-operator\" is invalid: spec.selector: Invalid value: v1.LabelSelector{...}, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable\n```\n\nThis error occurs because between version `1.0.0` and `1.0.1` a new `label` was added to the DataPower Operator Deployment resource to fix an issue related to operator-metrics. However, the install plan is not able to resolve this update because the LabelSelector is an immutable field.\n\n### Resolution\n\nTo workaround this issue, you can manually delete the existing `datapower-operator` Deployment resource. The Operator Lifecycle Manager should then recreate the `datapower-operator` Deployment resource with the `1.0.1` spec, and the install plan should succeed.\n\n```\noc delete deployment datapower-operator\n```\n\nOnce done, validate a new `datapower-operator` Deployment is created:\n\n```\noc get deployment\n```\n\nThen validate the `1.0.1` install plan succeeds by checking the `ClusterServiceVersion` resource:\n\n```\n$ oc get csv\nNAME                        DISPLAY                 VERSION   REPLACES                    PHASE\ndatapower-operator.v1.0.1   IBM DataPower Gateway   1.0.1     datapower-operator.v1.0.0   Succeeded\n```\n","fileAbsolutePath":"/home/travis/build/IBM/datapower-operator-doc/src/pages/troubleshooting/operator-upgrades.mdx"}}}}