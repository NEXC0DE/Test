{"componentChunkName":"component---src-pages-features-monitor-mdx","path":"/features/monitor/","result":{"pageContext":{"frontmatter":{"title":"DataPower Monitor","description":"Details about the DataPower Operator's monitor feature"},"relativePagePath":"/features/monitor.mdx","titleType":"page","MdxNode":{"id":"24583457-bf09-5a21-80c4-fd0a89d68c5a","children":[],"parent":"7508a28b-88b6-5fa9-93a2-37688a281668","internal":{"content":"---\ntitle: DataPower Monitor\ndescription: Details about the DataPower Operator's monitor feature\n---\n\n<PageDescription>\n\nThe DataPower Monitor provides valueable pod lifecycle event logging as well as ensuring the stability of peering among DataPower gateway pods.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Relation to DataPowerService</AnchorLink>\n  <AnchorLink>Pod Events</AnchorLink>\n  <AnchorLink>Gateway Peering Monitoring</AnchorLink>\n</AnchorLinks>\n\n## Relation to DataPowerService\n\nFor a given `DataPowerService` instance, there must be a matching `DataPowerMonitor` instance. This relationship or linkage is held through the `name` metadata property of each instance. The `DataPowerService` and `DataPowerMonitor` custom resource instances **must** share the same `name` to be considered linked.\n\n### Automatic creation\n\nDuring the reconciliation of a `DataPowerService` instance, the DataPower Operator will look in the same namespace for a matching `DataPowerMonitor` instance with the same `name`. If no instance is found, one will be created with default values.\n\nThe `DataPowerService` instance will own the `DataPowerMonitor` instance, and thus if the `DataPowerService` instance is deleted, the `DataPowerMonitor` will be garbage collected automatically.\n\n<InlineNotification>\n\nYou can modify the `DataPowerMonitor` instance spec once it's automatically created.\n\n</InlineNotification>\n\n### Creating a DataPowerMonitor manually\n\nIf you wish to control the lifecycle of the `DataPowerMonitor` resource and provide custom configuration at deploy-time, you can create the `DataPowerMonitor` instance yourself.\n\nThe following rules apply:\n\n- The `DataPowerMonitor` custom resource instance **must be created before** the matching `DataPowerService` custom resource instance.\n- The `name` of the `DataPowerMonitor` custom resource you create **must** match the `name` of the `DataPowerService` you intend to create.\n- The `DataPowerMonitor` instance **will not** be automatically cleaned up when the matching `DataPowerService` is deleted.\n\n## Pod Events\n\nThe `DataPowerMonitor` controller watches for Pod events from the cluster. When an event is received, it's inspected to determine if the associated Pod is managed by the linked `DataPowerService` custom resource instance. If the associated Pod is managed by the linked `DataPowerService` instance, then the Pod event is handled.\n\nWhen a Pod event is first received and handled, the `lastEvent` Status property will be set with the timestamp of the Pod event, and `workPending` will be set `true`.\n\nThe [`lifecycleDebounceMs`](/apis/datapowermonitor/v1beta2#lifecycledebouncems) property determines how much time should pass between Pod events before work will be performed. Once that time has elapsed, work will be completed.\n\nWhen work is in-progress (e.g., [Gateway Peering Monitoring](/features/monitor#gateway-peering-monitoring)), the `workInProgress` Status property will be set `true`, `workPending` set back to `false`, and `lastEvent` cleared.\n\nOnce work is complete, `workInProgress` will be set `false`.\n\nFor more details on the properties and status discussed, see the [API documentation](/apis/datapowermonitor/v1beta2).\n\n### Logging\n\nAll Pod events associated with the linked `DataPowerService` will be logged (at `info` level) with various metadata for troubleshooting purposes. The logs themselves can contain the following information:\n\n- Message (`msg`) will be one of:\n  - `Observed Pod event`\n  - `Warning: Pod failed to schedule`\n  - `Warning: Container is in waiting state`\n- `Monitor.Name`: the name of the associated `DataPowerMonitor`\n- `Pod.Name`: the name of the Pod that triggered the event\n- `Pod.Namespace`: the namespace in which the Pod resides\n- `Pod.UID`: the Pod's UID\n- `Pod.IP`: the Pod's IP address\n- `Reason`: reason associated event condition or container status\n- `Message`: message associated with event condition or container status\n\n## Gateway Peering Monitoring\n\nWhen a `DataPowerService` set of Pods are configured to use gateway peering (e.g. in the API Connect Gateway Service) and the associated `DataPowerMonitor` has `monitorGatewayPeering` enabled, the DataPower Operator will ensure that the gateway peering configurations remain stable.\n\nThe DataPower Monitor achieves this by responding to pod lifecycle events (e.g., when a DataPower pod is deleted or restarts) by examining the gateway peering status for every active pod. If a pod has \"stale peers\" (i.e., failed connections to peers that no longer exist), the DataPower Monitor will issue a maintanence command to reset its peering status. This will cause the pod to drop its connections to all of its peers, both active and inactive, at which point the active peers will reestablish their connections to the pod, and thus only connections to active peers will remain.\n\nThis process is all necessary because individual gateways do not always have enough information to determine when a peer has been removed permanently, and thus assistance from the Operator is needed.","type":"Mdx","contentDigest":"058d61d2a59c5f4f0cfbb67717d7d4c5","counter":116,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"DataPower Monitor","description":"Details about the DataPower Operator's monitor feature"},"exports":{},"rawBody":"---\ntitle: DataPower Monitor\ndescription: Details about the DataPower Operator's monitor feature\n---\n\n<PageDescription>\n\nThe DataPower Monitor provides valueable pod lifecycle event logging as well as ensuring the stability of peering among DataPower gateway pods.\n\n</PageDescription>\n\n<AnchorLinks>\n  <AnchorLink>Relation to DataPowerService</AnchorLink>\n  <AnchorLink>Pod Events</AnchorLink>\n  <AnchorLink>Gateway Peering Monitoring</AnchorLink>\n</AnchorLinks>\n\n## Relation to DataPowerService\n\nFor a given `DataPowerService` instance, there must be a matching `DataPowerMonitor` instance. This relationship or linkage is held through the `name` metadata property of each instance. The `DataPowerService` and `DataPowerMonitor` custom resource instances **must** share the same `name` to be considered linked.\n\n### Automatic creation\n\nDuring the reconciliation of a `DataPowerService` instance, the DataPower Operator will look in the same namespace for a matching `DataPowerMonitor` instance with the same `name`. If no instance is found, one will be created with default values.\n\nThe `DataPowerService` instance will own the `DataPowerMonitor` instance, and thus if the `DataPowerService` instance is deleted, the `DataPowerMonitor` will be garbage collected automatically.\n\n<InlineNotification>\n\nYou can modify the `DataPowerMonitor` instance spec once it's automatically created.\n\n</InlineNotification>\n\n### Creating a DataPowerMonitor manually\n\nIf you wish to control the lifecycle of the `DataPowerMonitor` resource and provide custom configuration at deploy-time, you can create the `DataPowerMonitor` instance yourself.\n\nThe following rules apply:\n\n- The `DataPowerMonitor` custom resource instance **must be created before** the matching `DataPowerService` custom resource instance.\n- The `name` of the `DataPowerMonitor` custom resource you create **must** match the `name` of the `DataPowerService` you intend to create.\n- The `DataPowerMonitor` instance **will not** be automatically cleaned up when the matching `DataPowerService` is deleted.\n\n## Pod Events\n\nThe `DataPowerMonitor` controller watches for Pod events from the cluster. When an event is received, it's inspected to determine if the associated Pod is managed by the linked `DataPowerService` custom resource instance. If the associated Pod is managed by the linked `DataPowerService` instance, then the Pod event is handled.\n\nWhen a Pod event is first received and handled, the `lastEvent` Status property will be set with the timestamp of the Pod event, and `workPending` will be set `true`.\n\nThe [`lifecycleDebounceMs`](/apis/datapowermonitor/v1beta2#lifecycledebouncems) property determines how much time should pass between Pod events before work will be performed. Once that time has elapsed, work will be completed.\n\nWhen work is in-progress (e.g., [Gateway Peering Monitoring](/features/monitor#gateway-peering-monitoring)), the `workInProgress` Status property will be set `true`, `workPending` set back to `false`, and `lastEvent` cleared.\n\nOnce work is complete, `workInProgress` will be set `false`.\n\nFor more details on the properties and status discussed, see the [API documentation](/apis/datapowermonitor/v1beta2).\n\n### Logging\n\nAll Pod events associated with the linked `DataPowerService` will be logged (at `info` level) with various metadata for troubleshooting purposes. The logs themselves can contain the following information:\n\n- Message (`msg`) will be one of:\n  - `Observed Pod event`\n  - `Warning: Pod failed to schedule`\n  - `Warning: Container is in waiting state`\n- `Monitor.Name`: the name of the associated `DataPowerMonitor`\n- `Pod.Name`: the name of the Pod that triggered the event\n- `Pod.Namespace`: the namespace in which the Pod resides\n- `Pod.UID`: the Pod's UID\n- `Pod.IP`: the Pod's IP address\n- `Reason`: reason associated event condition or container status\n- `Message`: message associated with event condition or container status\n\n## Gateway Peering Monitoring\n\nWhen a `DataPowerService` set of Pods are configured to use gateway peering (e.g. in the API Connect Gateway Service) and the associated `DataPowerMonitor` has `monitorGatewayPeering` enabled, the DataPower Operator will ensure that the gateway peering configurations remain stable.\n\nThe DataPower Monitor achieves this by responding to pod lifecycle events (e.g., when a DataPower pod is deleted or restarts) by examining the gateway peering status for every active pod. If a pod has \"stale peers\" (i.e., failed connections to peers that no longer exist), the DataPower Monitor will issue a maintanence command to reset its peering status. This will cause the pod to drop its connections to all of its peers, both active and inactive, at which point the active peers will reestablish their connections to the pod, and thus only connections to active peers will remain.\n\nThis process is all necessary because individual gateways do not always have enough information to determine when a peer has been removed permanently, and thus assistance from the Operator is needed.","fileAbsolutePath":"/home/travis/build/IBM/datapower-operator-doc/src/pages/features/monitor.mdx"}}}}